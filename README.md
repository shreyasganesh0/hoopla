# Hoopla: A Python RAG CLI

Hoopla is a command-line application built in Python to explore and demonstrate the core concepts of Retrieval-Augmented Generation (RAG). 

[Image of RAG process diagram]
 It allows users to query a knowledge base using different search strategies and receive answers generated by a Large Language Model (LLM) that has been provided with the retrieved context.

## Why Build This?

RAG is a powerful technique for making LLMs more accurate, relevant, and grounded in specific data. I built Hoopla to gain hands-on experience with:
* **Different Retrieval Strategies:** Implementing and comparing keyword, semantic (vector-based), and hybrid search methods.
* **Vector Embeddings & Databases:** (Assuming usage) Understanding how documents are converted into vectors and queried for similarity.
* **LLM Prompt Engineering:** Crafting effective prompts that instruct the LLM to synthesize an answer based *only* on the provided context.
* **Evaluation Metrics:** Developing ways to assess the quality of both the retrieval and the final generated answer.
* **Building AI Applications:** Integrating various components (retriever, LLM, prompt templates) into a cohesive application.

## Features

* **Modular CLI Interface:** Separate CLI commands for different functionalities (keyword search, semantic search, hybrid search, RAG, evaluation) [cite: shreyasganesh0/hoopla/hoopla-704fa1b51b864919c83d04e72847992193b871fc/cli/].
* **Keyword Search:** Basic text matching retrieval [cite: shreyasganesh0/hoopla/hoopla-704fa1b51b864919c83d04e72847992193b871fc/cli/lib/keyword_search.py].
* **Semantic Search:** (Likely uses vector embeddings) Finds documents based on conceptual similarity to the query [cite: shreyasganesh0/hoopla/hoopla-704fa1b51b864919c83d04e72847992193b871fc/cli/lib/semantic_search.py].
* **Hybrid Search:** Combines keyword and semantic search results, potentially using ranking algorithms like Reciprocal Rank Fusion (RRF) [cite: shreyasganesh0/hoopla/hoopla-704fa1b51b864919c83d04e72847992193b871fc/cli/lib/hybrid_search.py].
* **Retrieval-Augmented Generation (RAG):**
    * Retrieves context using one of the search methods.
    * Constructs a detailed prompt including the query and retrieved context [cite: shreyasganesh0/hoopla/hoopla-704fa1b51b864919c83d04e72847992193b871fc/cli/lib/llm_prompt.py].
    * Sends the prompt to an LLM (e.g., OpenAI API) to generate a grounded answer [cite: shreyasganesh0/hoopla/hoopla-704fa1b51b864919c83d04e72847992193b871fc/cli/lib/augmented_generation.py].
* **Evaluation Framework:** Includes tools to evaluate the performance of the RAG pipeline [cite: shreyasganesh0/hoopla/hoopla-704fa1b51b864919c83d04e72847992193b871fc/cli/lib/evaluation.py].

## Technical Deep Dive: The RAG Pipeline

The core RAG logic orchestrates the retrieval and generation steps. It takes a user query, uses a selected retriever (keyword, semantic, or hybrid) to find relevant documents/chunks, formats these into a context block, constructs a final prompt instructing the LLM, and calls the LLM API.

```python
# Conceptual snippet based on cli/lib/augmented_generation.py
# Replace with actual key function if different

from .llm_prompt import create_prompt # Assuming a prompt templating function
from .hybrid_search import search as hybrid_retriever # Or semantic/keyword
# Assume llm_client interacts with an LLM API (e.g., OpenAI)

def generate_answer(query: str, search_type: str = "hybrid", top_k: int = 5):
    """
    Performs Retrieval-Augmented Generation.
    """
    # 1. Retrieve Context
    if search_type == "hybrid":
        retrieved_docs = hybrid_retriever(query, top_k=top_k)
    # elif search_type == "semantic": ...
    # elif search_type == "keyword": ...
    else:
        raise ValueError("Invalid search type")

    context = "\n".join([doc.content for doc in retrieved_docs]) # Format context

    # 2. Create Prompt
    prompt = create_prompt(query, context)

    # 3. Call LLM for Generation
    try:
        response = llm_client.generate(prompt) # Fictional LLM client call
        answer = response.text # Extract the answer
    except Exception as e:
        print(f"Error calling LLM: {e}")
        answer = "Sorry, I couldn't generate an answer."

    return answer, retrieved_docs # Return answer and the sources used
(Please replace the conceptual snippet above with the actual core function from augmented_generation.py or similar, showing how context is retrieved and passed to the LLM prompt.)
```
## How to Run
Prerequisites: Python 3.11+, uv (or pip), API keys for LLM provider (e.g., OpenAI). (Add specific vector DB setup if needed).

Clone the repository: git clone https://github.com/shreyasganesh0/hoopla.git

Navigate into the directory: cd hoopla

Install dependencies:

```Bash
uv pip install -r requirements.txt # Or pip install -r requirements.txt
```

Set up environment variables (e.g., OPENAI_API_KEY).

Run the CLI:

```Bash

# Example: Perform RAG using hybrid search
python cli/augmented_generation_cli.py --query "Your question here" --search-type hybrid

# Example: Perform semantic search only
python cli/semantic_search_cli.py --query "Your question here"
```
